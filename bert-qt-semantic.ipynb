{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8188534,"sourceType":"datasetVersion","datasetId":4848927},{"sourceId":39281,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":33139}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-22T16:57:56.820208Z","iopub.status.busy":"2024-04-22T16:57:56.819522Z","iopub.status.idle":"2024-04-22T16:57:58.893395Z","shell.execute_reply":"2024-04-22T16:57:58.892610Z","shell.execute_reply.started":"2024-04-22T16:57:56.820176Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:01.028817Z","iopub.status.busy":"2024-04-22T16:58:01.028323Z","iopub.status.idle":"2024-04-22T16:58:02.890282Z","shell.execute_reply":"2024-04-22T16:58:02.889314Z","shell.execute_reply.started":"2024-04-22T16:58:01.028788Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA_PATH = 'train_split.json'\n# TEST_DATA_PATH  = '/kaggle/input/nlp-project-dataset/test_split.json'\nVAL_DATA_PATH   = 'validation_split.json'","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-04-22T16:58:02.995497Z","iopub.status.busy":"2024-04-22T16:58:02.994762Z","iopub.status.idle":"2024-04-22T16:58:02.999555Z","shell.execute_reply":"2024-04-22T16:58:02.998572Z","shell.execute_reply.started":"2024-04-22T16:58:02.995466Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def read_json(file):\n    with open(file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:04.904849Z","iopub.status.busy":"2024-04-22T16:58:04.904170Z","iopub.status.idle":"2024-04-22T16:58:04.909354Z","shell.execute_reply":"2024-04-22T16:58:04.908453Z","shell.execute_reply.started":"2024-04-22T16:58:04.904816Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA = read_json(TRAIN_DATA_PATH)\nVALIDATION_DATA = read_json(VAL_DATA_PATH)\nTEST_DATA = read_json(TEST_DATA_PATH)","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:06.770327Z","iopub.status.busy":"2024-04-22T16:58:06.769374Z","iopub.status.idle":"2024-04-22T16:58:10.954904Z","shell.execute_reply":"2024-04-22T16:58:10.954044Z","shell.execute_reply.started":"2024-04-22T16:58:06.770293Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:12.346166Z","iopub.status.busy":"2024-04-22T16:58:12.345417Z","iopub.status.idle":"2024-04-22T16:58:12.406494Z","shell.execute_reply":"2024-04-22T16:58:12.405292Z","shell.execute_reply.started":"2024-04-22T16:58:12.346134Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 0.5\nMAX_LEN = 32\nBATCH_SIZE = 32\nSEQ_LEN = 20\n# FOR THE SEQ_LEN = 20, READ THE DATA RETREIVAL FILE","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:13.686939Z","iopub.status.busy":"2024-04-22T16:58:13.686264Z","iopub.status.idle":"2024-04-22T16:58:13.691276Z","shell.execute_reply":"2024-04-22T16:58:13.690222Z","shell.execute_reply.started":"2024-04-22T16:58:13.686907Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Chinese_Article_Data(Dataset):\n    def __init__(self, file, tokenizer, max_len, seq_len):\n        self.data = file\n        self.tokenizer = tokenizer\n        self.max_len   = max_len\n        self.seq_len   = seq_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        news_article_input_ids   = torch.empty((self.seq_len,self.max_len))\n        news_article_attention_list = torch.empty((self.seq_len,self.max_len))\n        itr = 0\n        for sentence_idx in range(len(self.data[idx]['news_article'])):\n            news_article_instance = self.data[idx]['news_article'][itr]\n            encoding = self.tokenizer.encode_plus (\n                news_article_instance, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n            if (itr == self.seq_len):\n                break\n            \n        while(itr < self.seq_len):\n            encoding = self.tokenizer.encode_plus (\n                'NULL', \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n        \n        question = self.data[idx]['question_stem']\n        encoding = self.tokenizer.encode_plus (\n                question, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n        question_input_ids = encoding['input_ids']\n        question_attention_mask = encoding['attention_mask']\n        answer_option = torch.tensor([float(x.replace(',', '')) for x in self.data[idx]['answer_options']])\n        answer   = torch.tensor([1.0 if (x==self.data[idx]['ans']) else 0.0 for x in range(4)])\n#         print(itr)\n        return news_article_input_ids, news_article_attention_list, question_input_ids, question_attention_mask, answer, answer_option\n","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:26.676462Z","iopub.status.busy":"2024-04-22T16:58:26.675627Z","iopub.status.idle":"2024-04-22T16:58:26.691259Z","shell.execute_reply":"2024-04-22T16:58:26.690064Z","shell.execute_reply.started":"2024-04-22T16:58:26.676430Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def getloader(file = None, tokenizer = tokenizer, max_len = MAX_LEN, batch_size = BATCH_SIZE, seq_len = SEQ_LEN):\n    dataset = Chinese_Article_Data (file, tokenizer, max_len,seq_len)\n    loader = DataLoader(dataset = dataset, batch_size = batch_size, num_workers = 4, shuffle = False, pin_memory = True)\n    return loader","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-04-22T16:58:23.155896Z","iopub.status.busy":"2024-04-22T16:58:23.154974Z","iopub.status.idle":"2024-04-22T16:58:23.161377Z","shell.execute_reply":"2024-04-22T16:58:23.160303Z","shell.execute_reply.started":"2024-04-22T16:58:23.155862Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Chinese_answer_Model(nn.Module):\n    def __init__(self):\n        super(Chinese_answer_Model, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-chinese\", return_dict = True)\n        self.fc1 = nn.Linear(768*(SEQ_LEN+1), 4)\n        self.out = nn.Linear(8, 4)\n    \n    def forward(self, news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option):\n        output = torch.empty((news_article_input_ids.shape[0], 4)).to(device)\n        \n        for i in range(news_article_input_ids.shape[0]):\n            text_out = self.bert(input_ids = news_article_input_ids[i].long(),attention_mask= news_article_attention_mask[i])\n            question_out = self.bert(input_ids = question_input_ids[i].long(),attention_mask= question_attention_mask[i])\n            out = torch.cat((text_out.pooler_output, question_out.pooler_output), dim = 0)\n            out = out.view(-1)\n            out = self.fc1(out)\n            out = torch.relu(out)\n            out = torch.cat((out, answer_option[i]),dim = -1)\n            out = self.out(out)\n            \n            out = torch.softmax(out,dim = 0)\n            output[i] = out\n        \n        \n        return output","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:32.109237Z","iopub.status.busy":"2024-04-22T16:58:32.108493Z","iopub.status.idle":"2024-04-22T16:58:32.118877Z","shell.execute_reply":"2024-04-22T16:58:32.117677Z","shell.execute_reply.started":"2024-04-22T16:58:32.109204Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data_loader = getloader(TRAIN_DATA)\nvalidation_data_loader = getloader(VALIDATION_DATA)\ntest_data_loader = getloader(TEST_DATA)","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:39.707648Z","iopub.status.busy":"2024-04-22T16:58:39.706885Z","iopub.status.idle":"2024-04-22T16:58:39.713062Z","shell.execute_reply":"2024-04-22T16:58:39.712054Z","shell.execute_reply.started":"2024-04-22T16:58:39.707611Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = Chinese_answer_Model()\nmodel.to(device)\nmodel = nn.DataParallel(model)","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:42.139141Z","iopub.status.busy":"2024-04-22T16:58:42.138796Z","iopub.status.idle":"2024-04-22T16:58:42.769123Z","shell.execute_reply":"2024-04-22T16:58:42.768285Z","shell.execute_reply.started":"2024-04-22T16:58:42.139116Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def loss_fn (outputs, targets):\n    return nn.CrossEntropyLoss()(outputs, targets)\noptimizer = torch.optim.SGD(params = model.parameters(), lr = LEARNING_RATE)","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:47.214337Z","iopub.status.busy":"2024-04-22T16:58:47.213473Z","iopub.status.idle":"2024-04-22T16:58:47.220807Z","shell.execute_reply":"2024-04-22T16:58:47.219777Z","shell.execute_reply.started":"2024-04-22T16:58:47.214304Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"EPOCH = 5","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:59:07.336212Z","iopub.status.busy":"2024-04-22T16:59:07.335522Z","iopub.status.idle":"2024-04-22T16:59:07.340425Z","shell.execute_reply":"2024-04-22T16:59:07.339368Z","shell.execute_reply.started":"2024-04-22T16:59:07.336181Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, val_loader):\n    train_loss_list = []\n    val_loss_list = []\n    for epochs in range(EPOCH):\n        train_loss = 0\n        val_loss = 0\n        model.train()\n        for batch , data in tqdm(enumerate(data_loader, 0), unit=\"batch\", total=len(data_loader)):\n            news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer, answer_option = data\n            news_article_input_ids = news_article_input_ids.to(device)\n            news_article_attention_mask = news_article_attention_mask.to(device)\n            question_input_ids = question_input_ids.to(device)\n            question_attention_mask = question_attention_mask.to(device)\n            answer = answer.to(device)\n            optimizer.zero_grad()\n            outputs = model(news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option)\n            loss = loss_fn(outputs, answer)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        model.eval()\n        with torch.no_grad():\n            for batch , data in  tqdm(enumerate(val_loader, 0), unit=\"batch\", total=len(val_loader)):\n                news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer, answer_option = data\n                news_article_input_ids = news_article_input_ids.to(device)\n                news_article_attention_mask = news_article_attention_mask.to(device)\n                question_input_ids = question_input_ids.to(device)\n                question_attention_mask = question_attention_mask.to(device)\n                answer = answer.to(device)\n                outputs = model(news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option)\n                loss = loss_fn(outputs, answer)\n\n                val_loss += loss.item()\n                \n        print(f\"[{epochs+1}/{EPOCH}], Training Loss: {train_loss/len(data_loader)} Validation_loss: {val_loss/len(val_loader)}\")\n        train_loss_list.append(train_loss/len(data_loader))\n        val_loss_list.append(val_loss/len(val_loader))\n    return train_loss_list, val_loss_list","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:58:51.900498Z","iopub.status.busy":"2024-04-22T16:58:51.899677Z","iopub.status.idle":"2024-04-22T16:58:51.911817Z","shell.execute_reply":"2024-04-22T16:58:51.910622Z","shell.execute_reply.started":"2024-04-22T16:58:51.900465Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainLoss, valLoss = train(train_data_loader, validation_data_loader)","metadata":{"execution":{"iopub.execute_input":"2024-04-22T16:59:11.335668Z","iopub.status.busy":"2024-04-22T16:59:11.335242Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 1264/1264 [42:03<00:00,  2.00s/batch]\n\n100%|██████████| 542/542 [06:36<00:00,  1.37batch/s]"},{"name":"stdout","output_type":"stream","text":"[1/5], Training Loss: 1.0618610332566727 Validation_loss: 1.136973583720267\n"},{"name":"stderr","output_type":"stream","text":"\n\n 37%|███▋      | 464/1264 [15:29<26:38,  2.00s/batch]"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nepochs = [x for x in range(1, EPOCH+ 1)]\nplt.plot(epochs, trainLoss, label='Training Loss')\nplt.plot(epochs, valLoss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss vs Epochs and Validation Loss vs Epochs')\nplt.legend()\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.module.state_dict(), 'Rishav_NLP_R1_final.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport torch.optim\nfrom transformers import BertModel, BertTokenizer\nfrom torch import nn\nimport json\nfrom torch.utils.data import DataLoader, Dataset\n\nLEARNING_RATE = 5e-2\nMAX_LEN = 32\nBATCH_SIZE = 32\nSEQ_LEN = 20\n# FOR THE SEQ_LEN = 20, READ THE DATA RETREIVAL FILE\n\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n\ndef read_json(file):\n    with open(file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\nclass Chinese_answer_Model(nn.Module):\n    def __init__(self):\n        super(Chinese_answer_Model, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-chinese\", return_dict = True)\n        self.fc1 = nn.Linear(768*(SEQ_LEN+1), 4)\n        self.out = nn.Linear(8, 4)\n    \n    def forward(self, news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option):\n        output = torch.empty((news_article_input_ids.shape[0], 4)).to(device)\n        \n        for i in range(news_article_input_ids.shape[0]):\n            text_out = self.bert(input_ids = news_article_input_ids[i].long(),attention_mask= news_article_attention_mask[i])\n            question_out = self.bert(input_ids = question_input_ids[i].long(),attention_mask= question_attention_mask[i])\n            out = torch.cat((text_out.pooler_output, question_out.pooler_output), dim = 0)\n            out = out.view(-1)\n            out = self.fc1(out)\n            out = torch.relu(out)\n            out = torch.cat((out, answer_option[i]),dim = -1)\n            out = self.out(out)\n            \n            out = torch.softmax(out,dim = 0)\n            output[i] = out\n        \n        \n        return output\n\nclass Chinese_Article_Data(Dataset):\n    def __init__(self, file, tokenizer, max_len, seq_len):\n        self.data = file\n        self.tokenizer = tokenizer\n        self.max_len   = max_len\n        self.seq_len   = seq_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        news_article_input_ids   = torch.empty((self.seq_len,self.max_len))\n        news_article_attention_list = torch.empty((self.seq_len,self.max_len))\n        itr = 0\n        for sentence_idx in range(len(self.data[idx]['news_article'])):\n            news_article_instance = self.data[idx]['news_article'][itr]\n            encoding = self.tokenizer.encode_plus (\n                news_article_instance, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n            if (itr == self.seq_len):\n                break\n            \n        while(itr < self.seq_len):\n            encoding = self.tokenizer.encode_plus (\n                'NULL', \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n        \n        question = self.data[idx]['question_stem']\n        encoding = self.tokenizer.encode_plus (\n                question, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n        question_input_ids = encoding['input_ids']\n        question_attention_mask = encoding['attention_mask']\n        answer_option = torch.tensor([float(x.replace(',', '')) for x in self.data[idx]['answer_options']])\n        answer   = torch.tensor([1.0 if (x==self.data[idx]['ans']-1) else 0.0 for x in range(4)])\n#         print(itr)\n        return news_article_input_ids, news_article_attention_list, question_input_ids, question_attention_mask, answer, answer_option\n\ndef getloader(file = None, tokenizer = tokenizer, max_len = MAX_LEN, batch_size = BATCH_SIZE, seq_len = SEQ_LEN):\n    dataset = Chinese_Article_Data (file, tokenizer, max_len,seq_len)\n    loader = DataLoader(dataset = dataset, batch_size = batch_size, num_workers = 4, shuffle = False, pin_memory = True)\n    return loader\n\n    \n\n# defining the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n\nfrom sklearn.metrics import f1_score, accuracy_score,  classification_report\ndef evaluate(file_path,model):\n    # give the loaded file\n    with open(file_path, 'r') as f:\n        file = json.load(f)\n    \n    data = getloader(file)\n    \n    num_correct = 0\n    num_wrong= 0\n    for batch, d in enumerate(data):\n\n        \n        news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer, answer_option = d\n        news_article_input_ids = news_article_input_ids.to(device)\n        news_article_attention_mask = news_article_attention_mask.to(device)\n        question_input_ids = question_input_ids.to(device)\n        question_attention_mask = question_attention_mask.to(device)\n        answer = answer.to(device)\n        outputs = model(news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option)\n\n      \n        \n        target_indices = torch.argmax(answer, dim=1)\n        prediction_indices = torch.argmax(outputs, dim=1)\n\n        # Compare the indices to find correct and wrong predictions\n        num_correct += torch.sum(target_indices == prediction_indices).item()\n        num_wrong += torch.sum(target_indices != prediction_indices).item()\n    \n    print (f\"Accuracy : {num_correct/(num_correct+num_wrong)}\")\n\nload_model = Chinese_answer_Model()\n# Load the state_dict\nload_model.load_state_dict(torch.load('/kaggle/input/bert-qt-semantic/pytorch/bert-qt/1/Rishav_NLP_R1_final_trained.pth'))\nload_model.to(device)\nload_model = nn.DataParallel(load_model)\nfile_path = \"/kaggle/input/nlp-project-dataset/test_split.json\"\nevaluate(file_path, load_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T16:17:50.928858Z","iopub.execute_input":"2024-04-25T16:17:50.929200Z","iopub.status.idle":"2024-04-25T16:24:57.909710Z","shell.execute_reply.started":"2024-04-25T16:17:50.929175Z","shell.execute_reply":"2024-04-25T16:24:57.908445Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389b2b68188b4a99855f5c91e25da0cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7d384e66da441f6a0cc82f67a87ce2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ad5b7969e2441da48dc9256e4dc3b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8ea547d55045e482796deaa1c34bc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032b5065c49e46d58a1b511edf334f53"}},"metadata":{}},{"name":"stdout","text":"Accuracy : 0.48545504917578614\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}