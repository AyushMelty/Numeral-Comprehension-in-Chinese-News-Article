{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8188534,"sourceType":"datasetVersion","datasetId":4848927},{"sourceId":8212752,"sourceType":"datasetVersion","datasetId":4864008}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-24T10:27:46.034643Z","iopub.execute_input":"2024-04-24T10:27:46.035017Z","iopub.status.idle":"2024-04-24T10:27:46.040032Z","shell.execute_reply.started":"2024-04-24T10:27:46.034979Z","shell.execute_reply":"2024-04-24T10:27:46.038969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T10:27:46.04166Z","iopub.status.idle":"2024-04-24T10:27:46.133158Z","shell.execute_reply.started":"2024-04-24T10:27:46.042006Z","shell.execute_reply":"2024-04-24T10:27:46.132462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA_PATH = '/kaggle/input/nlp-project/train_split.json'\nTEST_DATA_PATH  = '/kaggle/input/nlp-project/NQuAD_test.json'\nVAL_DATA_PATH   = '/kaggle/input/nlp-project/validation_split.json'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-24T10:27:46.134413Z","iopub.execute_input":"2024-04-24T10:27:46.134688Z","iopub.status.idle":"2024-04-24T10:27:46.138734Z","shell.execute_reply.started":"2024-04-24T10:27:46.134666Z","shell.execute_reply":"2024-04-24T10:27:46.137931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_json(file):\n    with open(file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\ndef set_num_sentence(data):\n    num_sentence = 0\n    for i in range(len(data)):\n        sentences = []\n        for j in range(len(data[i]['sentences_containing_the_numeral_in_answer_options'])):\n            sentences += [k for k in data[i]['sentences_containing_the_numeral_in_answer_options'][j]]\n        data[i]['sentences_containing_the_numeral_in_answer_options'] = sentences\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATA = read_json(TRAIN_DATA_PATH)\nVALIDATION_DATA = read_json(VAL_DATA_PATH)\nTEST_DATA = read_json(TEST_DATA_PATH)\n\nset_num_sentence(TRAIN_DATA)\nset_num_sentence(VALIDATION_DATA)\nset_num_sentence(TEST_DATA)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 0.5\nMAX_LEN = 32\nBATCH_SIZE = 32\nSEQ_LEN = 20\n# FOR THE SEQ_LEN = 30, READ THE DATA RETREIVAL FILE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Chinese_Article_Data(Dataset):\n    def __init__(self, file, tokenizer, max_len, seq_len):\n        self.data = file\n        self.tokenizer = tokenizer\n        self.max_len   = max_len\n        self.seq_len   = seq_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        news_article_input_ids   = torch.empty((self.seq_len,self.max_len))\n        news_article_attention_list = torch.empty((self.seq_len,self.max_len))\n        itr = 0\n        KEY = 'sentences_containing_the_numeral_in_answer_options'\n        for sentence_idx in range(len(self.data[idx][KEY])):\n            news_article_instance = self.data[idx][KEY][itr]\n            encoding = self.tokenizer.encode_plus (\n                news_article_instance, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n            if (itr == self.seq_len):\n                break\n            \n        while(itr < self.seq_len):\n            encoding = self.tokenizer.encode_plus (\n                'NULL', \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n        \n        question = self.data[idx]['question_stem']\n        encoding = self.tokenizer.encode_plus (\n                question, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n        question_input_ids = encoding['input_ids']\n        question_attention_mask = encoding['attention_mask']\n        answer_option = torch.tensor([float(x.replace(',', '')) for x in self.data[idx]['answer_options']])\n        answer   = torch.tensor([1.0 if (x==self.data[idx]['ans']-1) else 0.0 for x in range(4)])\n#         print(itr)\n        return news_article_input_ids, news_article_attention_list, question_input_ids, question_attention_mask, answer, answer_option\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getloader(file = None, tokenizer = tokenizer, max_len = MAX_LEN, batch_size = BATCH_SIZE, seq_len = SEQ_LEN):\n    dataset = Chinese_Article_Data (file, tokenizer, max_len,seq_len)\n    loader = DataLoader(dataset = dataset, batch_size = batch_size, num_workers = 4, shuffle = False, pin_memory = True)\n    return loader","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Chinese_answer_Model(nn.Module):\n    def __init__(self):\n        super(Chinese_answer_Model, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-chinese\", return_dict = True)\n        self.fc1 = nn.Linear(768*(SEQ_LEN+1), 128)\n        self.fc2 = nn.Linear(128,64)\n        self.fc3 = nn.Linear(64,4)\n        self.out = nn.Linear(8, 4)\n    \n    def forward(self, news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option):\n        output = torch.empty((news_article_input_ids.shape[0], 4)).to(device)\n        \n        for i in range(news_article_input_ids.shape[0]):\n            text_out = self.bert(input_ids = news_article_input_ids[i].long(),attention_mask= news_article_attention_mask[i])\n            question_out = self.bert(input_ids = question_input_ids[i].long(),attention_mask= question_attention_mask[i])\n            out = torch.cat((text_out.pooler_output, question_out.pooler_output), dim = 0)\n            out = out.view(-1)\n            out = self.fc1(out)\n            out = torch.relu(out)\n            \n            out = self.fc2(out)\n            out = torch.relu(out)\n            \n            out = self.fc3(out)\n            out = torch.relu(out)\n            \n            out = torch.cat((out, answer_option[i]),dim = -1)\n            out = self.out(out)\n            \n            out = torch.softmax(out,dim = 0)\n            output[i] = out\n        \n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_loader = getloader(TRAIN_DATA)\nvalidation_data_loader = getloader(VALIDATION_DATA)\ntest_data_loader = getloader(TEST_DATA)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Chinese_answer_Model()\nmodel.to(device)\nmodel = nn.DataParallel(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn (outputs, targets):\n    return nn.CrossEntropyLoss()(outputs, targets)\noptimizer = torch.optim.SGD(params = model.parameters(), lr = LEARNING_RATE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, val_loader):\n    train_loss_list = []\n    val_loss_list = []\n    for epochs in range(EPOCH):\n        train_loss = 0\n        val_loss = 0\n        model.train()\n        for batch , data in tqdm(enumerate(data_loader, 0), unit=\"batch\", total=len(data_loader)):\n            news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer, answer_option = data\n            news_article_input_ids = news_article_input_ids.to(device)\n            news_article_attention_mask = news_article_attention_mask.to(device)\n            question_input_ids = question_input_ids.to(device)\n            question_attention_mask = question_attention_mask.to(device)\n            answer = answer.to(device)\n            optimizer.zero_grad()\n            outputs = model(news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option)\n            loss = loss_fn(outputs, answer)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        model.eval()\n        with torch.no_grad():\n            for batch , data in  tqdm(enumerate(val_loader, 0), unit=\"batch\", total=len(val_loader)):\n                news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer, answer_option = data\n                news_article_input_ids = news_article_input_ids.to(device)\n                news_article_attention_mask = news_article_attention_mask.to(device)\n                question_input_ids = question_input_ids.to(device)\n                question_attention_mask = question_attention_mask.to(device)\n                answer = answer.to(device)\n                outputs = model(news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option)\n                loss = loss_fn(outputs, answer)\n\n                val_loss += loss.item()\n                \n        print(f\"[{epochs+1}/{EPOCH}], Training Loss: {train_loss/len(data_loader)} Validation_loss: {val_loss/len(val_loader)}\")\n        torch.save(model.module.state_dict(), f'Rishav_NLP_R1_number_sentence-checkpoint-{epochs+1}.pth')\n        train_loss_list.append(train_loss/len(data_loader))\n        val_loss_list.append(val_loss/len(val_loader))\n    return train_loss_list, val_loss_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainLoss, valLoss = train(train_data_loader, validation_data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nepochs = [x for x in range(1, EPOCH+ 1)]\nplt.plot(epochs, trainLoss, label='Training Loss')\nplt.plot(epochs, valLoss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss vs Epochs and Validation Loss vs Epochs')\nplt.legend()\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.module.state_dict(), 'Rishav_NLP_R1_number_sentence.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport torch.optim\nfrom transformers import BertModel, BertTokenizer\nfrom torch import nn\nimport json\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, Dataset\n\nLEARNING_RATE = 4e-1\nMAX_LEN = 32\nBATCH_SIZE = 16\nSEQ_LEN = 30\n# FOR THE SEQ_LEN = 20, READ THE DATA RETREIVAL FILE\n\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n\ndef read_json(file):\n    with open(file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\nclass Chinese_answer_Model(nn.Module):\n    def __init__(self):\n        super(Chinese_answer_Model, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-chinese\", return_dict = True)\n        self.fc1 = nn.Linear(768*(SEQ_LEN+1), 128)\n        self.fc2 = nn.Linear(128,64)\n        self.fc3 = nn.Linear(64,4)\n        self.out = nn.Linear(8, 4)\n    \n    def forward(self, news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option):\n        output = torch.empty((news_article_input_ids.shape[0], 4)).to(device)\n        \n        for i in range(news_article_input_ids.shape[0]):\n            text_out = self.bert(input_ids = news_article_input_ids[i].long(),attention_mask= news_article_attention_mask[i])\n            question_out = self.bert(input_ids = question_input_ids[i].long(),attention_mask= question_attention_mask[i])\n            out = torch.cat((text_out.pooler_output, question_out.pooler_output), dim = 0)\n            out = out.view(-1)\n            out = self.fc1(out)\n            out = torch.relu(out)\n            \n            out = self.fc2(out)\n            out = torch.relu(out)\n            \n            out = self.fc3(out)\n            out = torch.relu(out)\n            \n            out = torch.cat((out, answer_option[i]),dim = -1)\n            out = self.out(out)\n            \n            out = torch.softmax(out,dim = 0)\n            output[i] = out\n        \n        \n        return output\n\nclass Chinese_Article_Data(Dataset):\n    def __init__(self, file, tokenizer, max_len, seq_len):\n        self.data = file\n        self.tokenizer = tokenizer\n        self.max_len   = max_len\n        self.seq_len   = seq_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        news_article_input_ids   = torch.empty((self.seq_len,self.max_len))\n        news_article_attention_list = torch.empty((self.seq_len,self.max_len))\n        itr = 0\n        KEY = 'sentences_containing_the_numeral_in_answer_options'\n        for sentence_idx in range(len(self.data[idx][KEY])):\n            news_article_instance = self.data[idx][KEY][itr]\n            encoding = self.tokenizer.encode_plus (\n                news_article_instance, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n            if (itr == self.seq_len):\n                break\n\n        while(itr < self.seq_len):\n            encoding = self.tokenizer.encode_plus (\n                'NULL', \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n            news_article_input_ids[itr] = encoding['input_ids']\n            news_article_attention_list[itr]  = encoding['attention_mask']\n            itr += 1\n\n        question = self.data[idx]['question_stem']\n        encoding = self.tokenizer.encode_plus (\n                question, \n                max_length = self.max_len,\n                pad_to_max_length = True,\n                truncation = True,\n                padding = 'max_length',\n                return_attention_mask= True,\n                return_tensors = 'pt',\n            )\n        question_input_ids = encoding['input_ids']\n        question_attention_mask = encoding['attention_mask']\n        answer_option = torch.tensor([float(x.replace(',', '')) for x in self.data[idx]['answer_options']])\n        answer   = torch.tensor([1.0 if (x==self.data[idx]['ans']-1) else 0.0 for x in range(4)])\n    #         print(itr)\n        return news_article_input_ids, news_article_attention_list, question_input_ids, question_attention_mask, answer, answer_option\n\n\ndef getloader(file = None, tokenizer = tokenizer, max_len = MAX_LEN, batch_size = BATCH_SIZE, seq_len = SEQ_LEN):\n    dataset = Chinese_Article_Data (file, tokenizer, max_len,seq_len)\n    loader = DataLoader(dataset = dataset, batch_size = batch_size, num_workers = 4, shuffle = False, pin_memory = True)\n    return loader\n\n    \n\n# defining the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n\nfrom sklearn.metrics import f1_score, accuracy_score,  classification_report\ndef evaluate(file_path,model):\n    # give the loaded file\n    with open(file_path, 'r') as f:\n        file = json.load(f)\n    \n    data = getloader(file)\n    \n    num_correct = 0\n    num_wrong= 0\n    for batch, d in tqdm(enumerate(data, 0), unit=\"batch\", total=len(data)):\n\n        \n        news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer, answer_option = d\n        news_article_input_ids = news_article_input_ids.to(device)\n        news_article_attention_mask = news_article_attention_mask.to(device)\n        question_input_ids = question_input_ids.to(device)\n        question_attention_mask = question_attention_mask.to(device)\n        answer = answer.to(device)\n        outputs = model(news_article_input_ids, news_article_attention_mask,question_input_ids, question_attention_mask, answer_option)\n\n      \n        \n        target_indices = torch.argmax(answer, dim=1)\n        prediction_indices = torch.argmax(outputs, dim=1)\n\n        # Compare the indices to find correct and wrong predictions\n        num_correct += torch.sum(target_indices == prediction_indices).item()\n        num_wrong += torch.sum(target_indices != prediction_indices).item()\n    \n    print (f\"Accuracy : {num_correct/(num_correct+num_wrong)}\")\n\nload_model = Chinese_answer_Model()\n# Load the state_dict\nload_model.load_state_dict(torch.load('/kaggle/working/Rishav_NLP_R1_number_sentence.pth'))\nload_model.to(device)\nload_model = nn.DataParallel(load_model)\nfile_path = \"/kaggle/input/nlp-project-dataset/test_split.json\"\nevaluate(file_path, load_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}